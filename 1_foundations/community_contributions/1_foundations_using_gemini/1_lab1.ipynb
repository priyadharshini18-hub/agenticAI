{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the start of your adventure in Agentic AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Are you ready for action??</h2>\n",
    "            <span style=\"color:#ff7800;\">Have you completed all the setup steps in the <a href=\"../setup/\">setup</a> folder?<br/>\n",
    "            Have you read the <a href=\"../README.md\">README</a>? Many common questions are answered here!<br/>\n",
    "            Have you checked out the guides in the <a href=\"../guides/01_intro.ipynb\">guides</a> folder?<br/>\n",
    "            Well in that case, you're ready!!\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">This code is a live resource - keep an eye out for my updates</h2>\n",
    "            <span style=\"color:#00bfff;\">I push updates regularly. As people ask questions or have problems, I add more examples and improve explanations. As a result, the code below might not be identical to the videos, as I've added more steps and better comments. Consider this like an interactive book that accompanies the lectures.<br/><br/>\n",
    "            I try to send emails regularly with important updates related to the course. You can find this in the 'Announcements' section of Udemy in the left sidebar. You can also choose to receive my emails via your Notification Settings in Udemy. I'm respectful of your inbox and always try to add value with my emails!\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And please do remember to contact me if I can help\n",
    "\n",
    "And I love to connect: https://www.linkedin.com/in/eddonner/\n",
    "\n",
    "\n",
    "### New to Notebooks like this one? Head over to the guides folder!\n",
    "\n",
    "Just to check you've already added the Python and Jupyter extensions to Cursor, if not already installed:\n",
    "- Open extensions (View >> extensions)\n",
    "- Search for python, and when the results show, click on the ms-python one, and Install it if not already installed\n",
    "- Search for jupyter, and when the results show, click on the Microsoft one, and Install it if not already installed  \n",
    "Then View >> Explorer to bring back the File Explorer.\n",
    "\n",
    "And then:\n",
    "1. Click where it says \"Select Kernel\" near the top right, and select the option called `.venv (Python 3.12.9)` or similar, which should be the first choice or the most prominent choice. You may need to choose \"Python Environments\" first.\n",
    "2. Click in each \"cell\" below, starting with the cell immediately below this text, and press Shift+Enter to run\n",
    "3. Enjoy!\n",
    "\n",
    "After you click \"Select Kernel\", if there is no option like `.venv (Python 3.12.9)` then please do the following:  \n",
    "1. On Mac: From the Cursor menu, choose Settings >> VS Code Settings (NOTE: be sure to select `VSCode Settings` not `Cursor Settings`);  \n",
    "On Windows PC: From the File menu, choose Preferences >> VS Code Settings(NOTE: be sure to select `VSCode Settings` not `Cursor Settings`)  \n",
    "2. In the Settings search bar, type \"venv\"  \n",
    "3. In the field \"Path to folder with a list of Virtual Environments\" put the path to the project root, like C:\\Users\\username\\projects\\agents (on a Windows PC) or /Users/username/projects/agents (on Mac or Linux).  \n",
    "And then try again.\n",
    "\n",
    "Having problems with missing Python versions in that list? Have you ever used Anaconda before? It might be interferring. Quit Cursor, bring up a new command line, and make sure that your Anaconda environment is deactivated:    \n",
    "`conda deactivate`  \n",
    "And if you still have any problems with conda and python versions, it's possible that you will need to run this too:  \n",
    "`conda config --set auto_activate_base false`  \n",
    "and then from within the Agents directory, you should be able to run `uv python list` and see the Python 3.12 version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's do an import. If you get an Import Error, double check that your Kernel is correct..\n",
    "\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next it's time to load the API keys into environment variables\n",
    "# If this returns false, see the next cell!\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait, did that just output `False`??\n",
    "\n",
    "If so, the most common reason is that you didn't save your `.env` file after adding the key! Be sure to have saved.\n",
    "\n",
    "Also, make sure the `.env` file is named precisely `.env` and is in the project root directory (`agents`)\n",
    "\n",
    "By the way, your `.env` file should have a stop symbol next to it in Cursor on the left, and that's actually a good thing: that's Cursor saying to you, \"hey, I realize this is a file filled with secret information, and I'm not going to send it to an external AI to suggest changes, because your keys should not be shown to anyone else.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Final reminders</h2>\n",
    "            <span style=\"color:#ff7800;\">1. If you're not confident about Environment Variables or Web Endpoints / APIs, please read Topics 3 and 5 in this <a href=\"../guides/04_technical_foundations.ipynb\">technical foundations guide</a>.<br/>\n",
    "            2. If you want to use AIs other than OpenAI, like Gemini, DeepSeek or Ollama (free), please see the first section in this <a href=\"../guides/09_ai_apis_and_ollama.ipynb\">AI APIs guide</a>.<br/>\n",
    "            3. If you ever get a Name Error in Python, you can always fix it immediately; see the last section of this <a href=\"../guides/06_python_foundations.ipynb\">Python Foundations guide</a> and follow both tutorials and exercises.<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins AIzaSyCq\n"
     ]
    }
   ],
   "source": [
    "# Check the key - if you're not using OpenAI, check whichever key you're using! Ollama doesn't need a key.\n",
    "\n",
    "import os\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "if gemini_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {gemini_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set - please head to the troubleshooting guide in the setup folder\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now - the all important import statement\n",
    "# If you get an import error - head over to troubleshooting in the Setup folder\n",
    "# Even for other LLM providers like Gemini, you still use this OpenAI import - see Guide 9 for why\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we'll create an instance of the OpenAI class\n",
    "# If you're not sure what it means to create an instance of a class - head over to the guides folder (guide 6)!\n",
    "# If you get a NameError - head over to the guides folder (guide 6)to learn about NameErrors - always instantly fixable\n",
    "# If you're not using OpenAI, you just need to slightly modify this - precise instructions are in the AI APIs guide (guide 9)\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of messages in the familiar OpenAI format\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is 2+2?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 = 4\n"
     ]
    }
   ],
   "source": [
    "# And now call it! Any problems, head to the troubleshooting guide\n",
    "# This uses GPT 4.1 nano, the incredibly cheap model\n",
    "# The APIs guide (guide 9) has exact instructions for using even cheaper or free alternatives to OpenAI\n",
    "# If you get a NameError, head to the guides folder (guide 6) to learn about NameErrors - always instantly fixable\n",
    "model = \"gemini-2.5-flash-preview-05-20\"\n",
    "response = gemini.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now - let's ask for a question:\n",
    "\n",
    "question = \"Please propose a hard, challenging question to assess someone's IQ. Respond only with the question.\"\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider the sequence:\n",
      "\n",
      "[(1, A), (2, B), (4, D), (7, G), (11, K), (16, P), ?]\n",
      "\n",
      "What is the next pair in the sequence, following the same underlying logic?\n"
     ]
    }
   ],
   "source": [
    "# ask it - this uses GPT 4.1 mini, still cheap but more powerful than nano\n",
    "\n",
    "response = gemini.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "question = response.choices[0].message.content\n",
    "\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form a new messages list\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's analyze the sequence by looking at the numbers and letters separately.\n",
      "\n",
      "**1. Analyze the Number Sequence:**\n",
      "The numbers in the pairs are: 1, 2, 4, 7, 11, 16.\n",
      "Let's find the difference between consecutive numbers:\n",
      "*   2 - 1 = 1\n",
      "*   4 - 2 = 2\n",
      "*   7 - 4 = 3\n",
      "*   11 - 7 = 4\n",
      "*   16 - 11 = 5\n",
      "\n",
      "The differences form a simple arithmetic progression: 1, 2, 3, 4, 5.\n",
      "Following this pattern, the next difference should be 6.\n",
      "So, the next number in the sequence will be 16 + 6 = 22.\n",
      "\n",
      "**2. Analyze the Letter Sequence:**\n",
      "The letters in the pairs are: A, B, D, G, K, P.\n",
      "Let's convert these letters to their corresponding position in the alphabet (A=1, B=2, C=3, etc.):\n",
      "*   A = 1\n",
      "*   B = 2\n",
      "*   D = 4\n",
      "*   G = 7\n",
      "*   K = 11\n",
      "*   P = 16\n",
      "\n",
      "Notice that the sequence of numbers representing the letters (1, 2, 4, 7, 11, 16) is exactly the same as the number sequence from the first part of each pair!\n",
      "\n",
      "Therefore, the alphabetical position of the next letter will be the next number in this sequence, which we calculated as 22.\n",
      "\n",
      "Now, we need to find the letter that corresponds to the 22nd position in the alphabet:\n",
      "A=1, B=2, C=3, D=4, E=5, F=6, G=7, H=8, I=9, J=10, K=11, L=12, M=13, N=14, O=15, P=16, Q=17, R=18, S=19, T=20, U=21, **V=22**.\n",
      "\n",
      "So, the next letter is V.\n",
      "\n",
      "**Conclusion:**\n",
      "Combining the next number and the next letter, the next pair in the sequence is (22, V).\n",
      "\n",
      "The final answer is $\\boxed{(22, V)}$.\n"
     ]
    }
   ],
   "source": [
    "# Ask it again\n",
    "\n",
    "response = gemini.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's analyze the sequence by looking at the numbers and letters separately.\n",
       "\n",
       "**1. Analyze the Number Sequence:**\n",
       "The numbers in the pairs are: 1, 2, 4, 7, 11, 16.\n",
       "Let's find the difference between consecutive numbers:\n",
       "*   2 - 1 = 1\n",
       "*   4 - 2 = 2\n",
       "*   7 - 4 = 3\n",
       "*   11 - 7 = 4\n",
       "*   16 - 11 = 5\n",
       "\n",
       "The differences form a simple arithmetic progression: 1, 2, 3, 4, 5.\n",
       "Following this pattern, the next difference should be 6.\n",
       "So, the next number in the sequence will be 16 + 6 = 22.\n",
       "\n",
       "**2. Analyze the Letter Sequence:**\n",
       "The letters in the pairs are: A, B, D, G, K, P.\n",
       "Let's convert these letters to their corresponding position in the alphabet (A=1, B=2, C=3, etc.):\n",
       "*   A = 1\n",
       "*   B = 2\n",
       "*   D = 4\n",
       "*   G = 7\n",
       "*   K = 11\n",
       "*   P = 16\n",
       "\n",
       "Notice that the sequence of numbers representing the letters (1, 2, 4, 7, 11, 16) is exactly the same as the number sequence from the first part of each pair!\n",
       "\n",
       "Therefore, the alphabetical position of the next letter will be the next number in this sequence, which we calculated as 22.\n",
       "\n",
       "Now, we need to find the letter that corresponds to the 22nd position in the alphabet:\n",
       "A=1, B=2, C=3, D=4, E=5, F=6, G=7, H=8, I=9, J=10, K=11, L=12, M=13, N=14, O=15, P=16, Q=17, R=18, S=19, T=20, U=21, **V=22**.\n",
       "\n",
       "So, the next letter is V.\n",
       "\n",
       "**Conclusion:**\n",
       "Combining the next number and the next letter, the next pair in the sequence is (22, V).\n",
       "\n",
       "The final answer is $\\boxed{(22, V)}$."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(answer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "That was a small, simple step in the direction of Agentic AI, with your new environment!\n",
    "\n",
    "Next time things get more interesting..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Now try this commercial application:<br/>\n",
    "            First ask the LLM to pick a business area that might be worth exploring for an Agentic AI opportunity.<br/>\n",
    "            Then ask the LLM to present a pain-point in that industry - something challenging that might be ripe for an Agentic solution.<br/>\n",
    "            Finally have 3 third LLM call propose the Agentic AI solution. <br/>\n",
    "            We will cover this at up-coming labs, so don't worry if you're unsure.. just give it a try!\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's pick a high-leverage business area with significant existing pain points and a clear path for Agentic AI to deliver transformative value.\n",
       "\n",
       "---\n",
       "\n",
       "### Business Area: Autonomous Software Engineering & Development Agents\n",
       "\n",
       "**The Opportunity:**\n",
       "\n",
       "The field of software development is ripe for an Agentic AI revolution. While we have impressive tools like GitHub Copilot (for code generation) and various automated testing frameworks, the *entire process* from understanding requirements to deploying and maintaining production code still involves immense manual effort, coordination, and debugging.\n",
       "\n",
       "An Agentic AI in this space would go beyond mere assistance; it would take on defined goals (e.g., \"implement feature X,\" \"fix bug Y,\" \"refactor module Z\") and autonomously plan, execute, test, debug, and potentially even deploy the necessary code changes.\n",
       "\n",
       "**Why Agentic AI is a Game-Changer Here:**\n",
       "\n",
       "Current AI tools in software dev are mostly *reactive* (e.g., suggest code based on what you're typing) or *task-specific* (e.g., run tests). An Agentic AI would be **proactive, goal-oriented, and capable of multi-step reasoning and action initiation**.\n",
       "\n",
       "1.  **Autonomous Feature Implementation:**\n",
       "    *   **Agentic Aspect:** An agent could receive a user story or high-level specification, break it down into smaller tasks, write the necessary code (frontend, backend, database migrations), generate unit and integration tests, and then execute those tests. It would iterate, debugging its own code until tests pass and the feature meets the specified criteria.\n",
       "    *   **Value:** Dramatically reduced time-to-market for new features, lower development costs.\n",
       "\n",
       "2.  **Proactive Bug Fixing & Refactoring:**\n",
       "    *   **Agentic Aspect:** An agent could continuously monitor application logs, error reports, and performance metrics. When an anomaly or bug is detected, it could autonomously:\n",
       "        *   Analyze the logs to pinpoint the root cause.\n",
       "        *   Generate a hypothesis for a fix.\n",
       "        *   Write and test the patch.\n",
       "        *   Propose the fix for human review or even deploy it directly (with robust validation).\n",
       "        *   Identify code smells or performance bottlenecks and suggest/implement refactorings.\n",
       "    *   **Value:** Faster bug resolution, improved code quality, reduced technical debt, increased application stability.\n",
       "\n",
       "3.  **Intelligent Test Generation & Maintenance:**\n",
       "    *   **Agentic Aspect:** Beyond simple unit tests, an agent could:\n",
       "        *   Analyze existing code and requirements to generate comprehensive integration and end-to-end tests.\n",
       "        *   Automatically update tests when underlying code changes, preventing test rot.\n",
       "        *   Identify critical paths and areas of the application that are under-tested.\n",
       "        *   Generate synthetic data for testing edge cases.\n",
       "    *   **Value:** Higher code coverage, fewer regressions, more robust software.\n",
       "\n",
       "4.  **Automated Security Auditing & Remediation:**\n",
       "    *   **Agentic Aspect:** An agent could continuously scan codebases for security vulnerabilities (e.g., SQL injection, XSS, insecure dependencies). Upon detection, it could not only report the issue but also:\n",
       "        *   Suggest specific code changes to remediate the vulnerability.\n",
       "        *   Implement those changes.\n",
       "        *   Generate new security tests to ensure the fix holds.\n",
       "    *   **Value:** Proactive security posture, significantly reducing the attack surface and compliance burden.\n",
       "\n",
       "**Why this area is \"Worth Exploring\":**\n",
       "\n",
       "*   **Massive Market:** Every company that builds software is a potential customer. This includes tech giants, startups, and enterprises across all industries.\n",
       "*   **High Leverage:** Software underpins almost every modern business. Improving the efficiency and quality of its creation has exponential returns.\n",
       "*   **Clear ROI:** Reduced development costs, faster delivery cycles, fewer bugs, improved security, and higher developer productivity are all easily quantifiable benefits.\n",
       "*   **Human-in-the-Loop Potential:** Early iterations can focus on suggestions and automated drafts, gradually increasing autonomy as trust and capabilities grow. Human oversight for critical decisions will remain important, especially for deployment.\n",
       "*   **Existing Infrastructure:** Agentic AI can plug into existing Git repositories, CI/CD pipelines, IDEs, and issue trackers, making adoption feasible.\n",
       "\n",
       "**Challenges & Considerations:**\n",
       "\n",
       "*   **Trust and Reliability:** Developers need to trust the agent's output. Errors introduced by an agent can be costly. Robust testing, validation, and transparency will be crucial.\n",
       "*   **Complexity of Real-World Systems:** Handling legacy code, complex architectural decisions, and nuanced business logic will be difficult.\n",
       "*   **Security:** An agent that can modify and deploy code is a potential security risk if compromised.\n",
       "*   **Ethics:** Who is responsible when an AI-generated bug causes a problem?\n",
       "*   **Explainability:** Developers will need to understand *why* an agent made certain decisions or wrote code in a particular way.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "Autonomous Software Engineering & Development Agents represent a frontier where Agentic AI can profoundly impact productivity, quality, and innovation across the entire digital economy. It's an area with high demand, clear pain points, and the potential for multi-billion dollar solutions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First create the messages:\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Pick a business area that might be worth exploring for an Agentic AI opportunity.\"}]\n",
    "\n",
    "# Then make the first call:\n",
    "\n",
    "response = gemini.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# Then read the business idea:\n",
    "\n",
    "business_idea = response.choices[0].message.content\n",
    "\n",
    "\n",
    "display(Markdown(business_idea))\n",
    "\n",
    "# And repeat! In the next message, include the business idea within the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's zero in on a critical and pervasive pain point within the \"Autonomous Software Engineering & Development Agents\" domain:\n",
       "\n",
       "---\n",
       "\n",
       "### High-Leverage Pain Point: The Reactive and Manual Cycle of Production Bug Detection, Diagnosis, and Remediation\n",
       "\n",
       "**The Core Problem:**\n",
       "\n",
       "Modern software systems are inherently complex, distributed, and constantly evolving. Despite best efforts in testing, bugs inevitably escape into production. The current process for handling these post-deployment issues is overwhelmingly **reactive, manual, and resource-intensive**, leading to significant business costs, reputational damage, and developer burnout.\n",
       "\n",
       "**Elaboration of the Pain:**\n",
       "\n",
       "1.  **Delayed Detection & User Impact:**\n",
       "    *   **Pain:** Most production bugs are first detected by end-users, customers, or through generic error reporting tools *after* they've caused an impact (e.g., lost transactions, data corruption, service outages, poor user experience).\n",
       "    *   **Consequence:** Direct financial loss, customer dissatisfaction, brand damage, churn.\n",
       "\n",
       "2.  **Labor-Intensive Diagnosis & Root Cause Analysis:**\n",
       "    *   **Pain:** Once a bug is reported, highly skilled and expensive engineers (often on-call) must manually sift through mountains of logs, metrics, traces, and code. This involves:\n",
       "        *   Reproducing the issue (which can be difficult and non-deterministic).\n",
       "        *   Correlating disparate data points across microservices, databases, and third-party APIs.\n",
       "        *   Formulating hypotheses for the root cause.\n",
       "        *   Deep diving into specific code paths.\n",
       "    *   **Consequence:** This process is incredibly time-consuming, often taking hours or even days, diverting engineers from feature development. It's mentally taxing and prone to misdiagnosis, leading to \"chasing ghosts\" or implementing ineffective fixes.\n",
       "\n",
       "3.  **Manual Remediation, Testing, and Deployment:**\n",
       "    *   **Pain:** After diagnosis, an engineer must manually write the fix, develop or update unit/integration tests for that fix, go through code review, and then shepherd the change through a CI/CD pipeline for deployment.\n",
       "    *   **Consequence:** Even after diagnosis, the time-to-resolution (MTTR) remains high due to these manual steps, prolonging the impact of the bug. It also adds to technical debt if fixes are rushed or not thoroughly tested.\n",
       "\n",
       "4.  **Knowledge Silos & Institutional Memory Loss:**\n",
       "    *   **Pain:** Debugging expertise often resides in specific individuals. When those individuals leave or are unavailable, the institutional knowledge required to diagnose complex issues is lost or becomes harder to access.\n",
       "    *   **Consequence:** Increased MTTR, reliance on a few \"heroes,\" and decreased team resilience.\n",
       "\n",
       "5.  **Reactive & Opportunistic Refactoring (or lack thereof):**\n",
       "    *   **Pain:** Engineers are often too busy fighting fires to proactively identify and address underlying code quality issues, architectural flaws, or performance bottlenecks that contribute to recurring bugs. Refactoring often happens only when a module breaks or becomes utterly unmaintainable.\n",
       "    *   **Consequence:** Accumulation of technical debt, making the system brittle, harder to extend, and more prone to future bugs.\n",
       "\n",
       "**Why This is a Deep-Seated Pain (and ripe for Agentic AI):**\n",
       "\n",
       "This pain point isn't solved by simple automation scripts because it requires:\n",
       "*   **Contextual Understanding:** Comprehending not just code, but also system architecture, deployment environments, user behavior, and business logic.\n",
       "*   **Multi-Step Reasoning:** Moving from raw data (logs, errors) to an inferred problem, then to a hypothesis about the root cause, then to a specific code modification, then to validation.\n",
       "*   **Goal-Oriented Iteration:** The \"goal\" is a stable, bug-free system. This requires continuous monitoring, diagnosing, fixing, testing, and learning from failures.\n",
       "*   **Proactive Capabilities:** Moving beyond reacting to *known* errors to predicting or preventing them, and identifying opportunities for system improvement.\n",
       "*   **Adaptability:** Production systems are dynamic. An effective solution must adapt to new code, new services, and changing traffic patterns.\n",
       "\n",
       "**The Agentic AI Opportunity:**\n",
       "\n",
       "An Agentic AI solution would transform this reactive, manual cycle into a **proactive, autonomous, and self-healing process**:\n",
       "\n",
       "*   **Continuous Observability & Proactive Anomaly Detection:** An agent would continuously monitor application logs, metrics, traces, and user feedback systems. Using advanced pattern recognition and anomaly detection, it could identify emerging issues *before* they impact many users or escalate into critical failures.\n",
       "*   **Intelligent Root Cause Analysis (RCA):** Upon detection, the agent would autonomously collect and correlate data from all relevant sources (frontend errors, backend logs, database queries, infrastructure metrics). It would leverage large language models (LLMs) fine-tuned on codebases and debugging patterns to formulate probable root cause hypotheses, simulating an expert engineer's diagnostic process.\n",
       "*   **Automated Remediation Planning & Code Generation:** Based on the RCA, the agent would generate a proposed fix. This could involve:\n",
       "    *   Identifying the specific lines of code or configuration changes needed.\n",
       "    *   Suggesting architectural refactorings for recurring issues.\n",
       "    *   Generating unit, integration, and even end-to-end tests to validate the fix and prevent regressions.\n",
       "*   **Autonomous Iteration & Self-Correction:** The agent would run the generated tests. If they fail, it would autonomously analyze the test failures, debug its own code, iterate on the fix, and re-run tests until all pass and the issue is resolved according to defined criteria.\n",
       "*   **Proactive Deployment or Review:** For high-confidence fixes, the agent could propose the change for human review, or even, in highly trusted scenarios and sandboxed environments, automatically deploy the fix.\n",
       "*   **Continuous Learning:** Each successful diagnosis and fix would become a new data point, improving the agent's accuracy, efficiency, and ability to handle novel problems over time. The agent could also learn to identify \"code smells\" or patterns that lead to common bugs, suggesting proactive refactorings.\n",
       "\n",
       "**Quantifiable Impact:**\n",
       "\n",
       "Addressing this pain point with Agentic AI would lead to:\n",
       "\n",
       "*   **Massively Reduced Mean Time To Resolution (MTTR):** From hours/days to minutes.\n",
       "*   **Significant Cost Savings:** Less reliance on expensive on-call engineers for reactive debugging.\n",
       "*   **Improved Customer Satisfaction & Uptime:** Fewer production incidents, faster recovery.\n",
       "*   **Higher Developer Productivity:** Engineers freed from fire-fighting can focus on innovation and feature development.\n",
       "*   **Enhanced Code Quality & Reduced Technical Debt:** Proactive identification and remediation of issues.\n",
       "\n",
       "This pain point directly attacks the core inefficiencies and stresses of operating complex software systems, making it a prime candidate for transformative Agentic AI solutions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": f\"Present a pain-point in that {business_idea} industry - something challenging that might be ripe for an Agentic solution.\"}]\n",
    "\n",
    "response = gemini.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "pain_point = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(pain_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The critical pain point: **\"The Reactive and Manual Cycle of Production Bug Detection, Diagnosis, and Remediation\"** is indeed a prime target for Agentic AI. It demands continuous monitoring, multi-step reasoning, iterative problem-solving, and contextual understanding â€“ all hallmarks of advanced agent systems.\n",
       "\n",
       "Here's a proposal for an Agentic AI solution, let's call it **\"The Autonomic Bug Resolution Engine (ABRE)\"**.\n",
       "\n",
       "---\n",
       "\n",
       "## The Autonomic Bug Resolution Engine (ABRE)\n",
       "\n",
       "**Core Vision:** To transform the reactive, manual, and resource-intensive process of production bug handling into a proactive, autonomous, and self-healing lifecycle, significantly reducing MTTR (Mean Time To Resolution) and improving system resilience.\n",
       "\n",
       "---\n",
       "\n",
       "### Agentic Architecture: The ABRE Council of Agents\n",
       "\n",
       "The ABRE operates as a collaborative multi-agent system, where specialized agents work together, communicate, and iterate towards a common goal: a stable, bug-free production environment. Each agent has specific roles, access to tools, and a shared understanding of the system's state.\n",
       "\n",
       "1.  **Sentinel Agent (Observability & Anomaly Detection)**\n",
       "    *   **Role:** The \"eyes and ears\" of the system. Continuously monitors all observable data streams.\n",
       "    *   **Capabilities:**\n",
       "        *   Integrates with existing observability platforms (logs, metrics, traces, APM, user feedback systems like Sentry/Datadog/New Relic).\n",
       "        *   Applies advanced AI/ML for anomaly detection (e.g., unusual error rates, performance degradation, specific error message patterns, unusual user behavior).\n",
       "        *   Contextualizes anomalies using historical data and baseline metrics.\n",
       "    *   **Tools:** Observability platform APIs, Time-series databases, Machine Learning models for anomaly detection.\n",
       "    *   **Initiates:** Triggers the process by creating an \"Incident Report\" and routing it to the Pathfinder Agent.\n",
       "\n",
       "2.  **Pathfinder Agent (Diagnosis & Root Cause Analysis - RCA)**\n",
       "    *   **Role:** The \"detective.\" Takes an incident report and autonomously investigates the root cause.\n",
       "    *   **Capabilities:**\n",
       "        *   **Contextual Data Correlation:** Gathers all relevant data related to the incident from various sources (logs from Sentinel, code structure from VCS, architectural diagrams from internal docs, dependency maps).\n",
       "        *   **Hypothesis Generation:** Leverages an LLM fine-tuned on codebases, error patterns, and past incident reports to formulate multiple root cause hypotheses.\n",
       "        *   **Trace Analysis:** Follows execution paths through distributed tracing data, identifying faulty services or code paths.\n",
       "        *   **Query Generation:** Can generate targeted queries for databases, caches, or specific service logs to validate hypotheses.\n",
       "    *   **Tools:** VCS APIs (Git), Observability platform APIs, Internal documentation APIs (Confluence/Wiki), Graph databases (for dependency mapping), LLM with RAG capabilities (retrieval-augmented generation for contextual information).\n",
       "    *   **Outputs:** A prioritized list of probable root causes, impacted components, and a high-level proposed remediation strategy (e.g., \"fix `UserService.java` line 123 to handle null pointer\").\n",
       "\n",
       "3.  **Craftsman Agent (Code Generation & Remediation)**\n",
       "    *   **Role:** The \"engineer.\" Translates the Pathfinder's diagnosis into concrete code changes and associated tests.\n",
       "    *   **Capabilities:**\n",
       "        *   **Code Generation:** Based on the RCA and proposed strategy, generates specific code modifications (e.g., patch, new function, configuration change).\n",
       "        *   **Test Generation:** Automatically generates new unit, integration, and even relevant end-to-end tests to validate the fix and prevent regressions.\n",
       "        *   **Refactoring Suggestions:** For recurring issues or identified \"code smells,\" it can suggest or implement minor refactorings as part of the fix.\n",
       "        *   **Iterative Development:** Capable of receiving feedback from the Guardian Agent (test failures) and autonomously iterating on its generated code until tests pass.\n",
       "    *   **Tools:** Code generation LLM (e.g., fine-tuned GPT, Codex), VCS APIs (for cloning, branching, committing), IDE-like capabilities (static analysis, linting).\n",
       "    *   **Outputs:** A proposed Pull Request (PR) containing the code fix and new test cases.\n",
       "\n",
       "4.  **Guardian Agent (Validation & Testing)**\n",
       "    *   **Role:** The \"quality assurance.\" Ensures the proposed fix is correct, safe, and doesn't introduce regressions.\n",
       "    *   **Capabilities:**\n",
       "        *   **Test Execution:** Executes all generated tests (unit, integration) and a subset of the existing regression test suite in a sandboxed, isolated environment.\n",
       "        *   **Performance & Security Checks:** Runs performance benchmarks and basic security scans on the changed code.\n",
       "        *   **Rollback Strategy Simulation:** Simulates potential issues or rollbacks in case of post-deployment failure.\n",
       "        *   **Detailed Feedback:** Provides granular feedback to the Craftsman Agent if tests fail (e.g., \"test `UserServiceTest.testNullUser` failed with `NullPointerException` on line 45\").\n",
       "    *   **Tools:** CI/CD pipeline APIs, Test automation frameworks (JUnit, Selenium, Playwright), Performance testing tools, Static Application Security Testing (SAST) tools, Containerization (Docker, Kubernetes) for isolated environments.\n",
       "    *   **Outputs:** Test results (Pass/Fail), performance metrics, security scan reports.\n",
       "\n",
       "5.  **Chronicler Agent (Knowledge Management & Continuous Learning)**\n",
       "    *   **Role:** The \"historian and mentor.\" Learns from every incident, successful fix, and failure.\n",
       "    *   **Capabilities:**\n",
       "        *   **Knowledge Base Enrichment:** Documents the entire incident lifecycle, including the initial anomaly, Pathfinder's diagnosis, Craftsman's fix, Guardian's validation, and post-deployment monitoring results.\n",
       "        *   **Pattern Recognition:** Identifies recurring bug patterns, common root causes, and effective remediation strategies.\n",
       "        *   **Proactive Insights:** Suggests proactive refactorings, architectural improvements, or additional monitoring points based on learned patterns to the Steward Agent or human developers.\n",
       "        *   **Agent Fine-tuning:** Uses collected data to continuously fine-tune the LLMs and algorithms used by other agents, improving their accuracy and efficiency.\n",
       "    *   **Tools:** Vector databases (for semantic search of incidents), ML models for pattern recognition, Observability platform APIs (for post-fix validation).\n",
       "    *   **Feeds:** Information back to Pathfinder (for similar past issues), Craftsman (for best practices in fixes), and Sentinel (for new anomaly patterns).\n",
       "\n",
       "6.  **Steward Agent (Orchestration & Deployment/Review)**\n",
       "    *   **Role:** The \"manager.\" Oversees the entire workflow, ensures smooth transitions between agents, and handles external interactions.\n",
       "    *   **Capabilities:**\n",
       "        *   **Workflow Orchestration:** Manages the state of the incident, delegates tasks to appropriate agents, and monitors their progress.\n",
       "        *   **Human-in-the-Loop Integration:** For critical fixes or in environments with strict change control, prepares a comprehensive Pull Request with all relevant context (RCA, proposed fix, test results, performance impact) for human review and approval.\n",
       "        *   **Autonomous Deployment:** For high-confidence, pre-approved types of fixes (e.g., minor configuration updates, non-breaking patches in specific services), can trigger the CI/CD pipeline for autonomous deployment.\n",
       "        *   **Communication:** Notifies relevant teams/individuals about incident status, progress, and resolution.\n",
       "    *   **Tools:** Workflow orchestration engine (e.g., temporal, custom), VCS APIs (for PR creation), CI/CD pipeline APIs, Communication APIs (Slack, Teams, Jira).\n",
       "    *   **Outputs:** Deployed fix, or a ready-for-review PR, and updated incident status.\n",
       "\n",
       "---\n",
       "\n",
       "### Workflow: Autonomous Bug Resolution Scenario\n",
       "\n",
       "1.  **Detection (Sentinel Agent):** Sentinel detects a sudden spike in 5xx errors from the `PaymentService` in production, exceeding a predefined threshold, and correlates it with a recent deployment of a new feature. It flags this as a critical incident and creates an `Incident_001` report, passing initial context to the Pathfinder.\n",
       "\n",
       "2.  **Diagnosis (Pathfinder Agent):**\n",
       "    *   Pathfinder receives `Incident_001`.\n",
       "    *   It queries Sentinel for granular logs and traces for the `PaymentService` errors.\n",
       "    *   It pulls recent code changes from Git for `PaymentService` and dependent services.\n",
       "    *   Leveraging its RAG-LLM, it searches the Chronicler's knowledge base for similar past `PaymentService` errors.\n",
       "    *   It correlates specific log messages (`NullPointerException` in `PaymentService.java:123` during `processPayment` method) with the recent code change.\n",
       "    *   **Output:** Determines the root cause is a missing null check when processing a specific payment type introduced in the latest commit to `PaymentService.java`, impacting a small percentage of transactions. Proposed fix strategy: Add null check and default value.\n",
       "\n",
       "3.  **Remediation (Craftsman Agent):**\n",
       "    *   Craftsman receives the Pathfinder's diagnosis and proposed fix strategy.\n",
       "    *   It fetches `PaymentService.java` from Git.\n",
       "    *   It generates code to add the necessary null check and implements a default behavior for the problematic payment type.\n",
       "    *   It then generates a new unit test `PaymentServiceTest.testNullPaymentTypeHandling()` to specifically cover this edge case, and updates an integration test.\n",
       "    *   **Output:** Creates a temporary branch `fix/incident-001` and commits the code and test changes.\n",
       "\n",
       "4.  **Validation (Guardian Agent):**\n",
       "    *   Guardian receives the new branch and proposed changes.\n",
       "    *   It spins up an isolated sandbox environment with the `PaymentService` and its dependencies.\n",
       "    *   It executes the newly generated tests and a subset of the existing `PaymentService` regression suite.\n",
       "    *   **Scenario 1 (Failure):** The new unit test fails, indicating the fix isn't entirely correct or introduces a new issue. Guardian sends detailed failure logs back to Craftsman. Craftsman autonomously refines the code, and Guardian re-runs tests until they pass.\n",
       "    *   **Scenario 2 (Success):** All tests pass. Guardian confirms no regressions and reports success to the Steward.\n",
       "\n",
       "5.  **Knowledge Capture (Chronicler Agent):**\n",
       "    *   Throughout this process, Chronicler observes and records every step: the initial anomaly, Pathfinder's analysis, Craftsman's iterations, Guardian's test results, and the final approved fix. It updates its knowledge base, tagging the incident with relevant code patterns and service names. This information will improve future diagnoses and code generation.\n",
       "\n",
       "6.  **Deployment/Review (Steward Agent):**\n",
       "    *   Steward receives confirmation from Guardian that all tests passed.\n",
       "    *   Based on the criticality (minor bug, isolated impact) and pre-defined policies for `PaymentService` (e.g., auto-deploy minor patches), Steward autonomously initiates the CI/CD pipeline to deploy `fix/incident-001` to production.\n",
       "    *   Steward updates the `Incident_001` status to \"Resolved\" in the issue tracker and notifies the relevant on-call team.\n",
       "\n",
       "7.  **Post-Deployment Monitoring (Sentinel Agent):** Sentinel continues to monitor `PaymentService` to confirm the error rate returns to normal and no new anomalies appear, validating the fix. If issues persist, the cycle restarts.\n",
       "\n",
       "---\n",
       "\n",
       "### Quantifiable Impact & Value Proposition:\n",
       "\n",
       "*   **Massively Reduced Mean Time To Resolution (MTTR):** From hours/days of manual effort to minutes/single-digit hours of autonomous resolution.\n",
       "*   **Significant Cost Savings:** Drastically lowers reliance on expensive on-call engineers for reactive debugging, freeing them for innovation.\n",
       "*   **Improved Customer Satisfaction & Uptime:** Proactive detection and rapid resolution lead to fewer outages and a better user experience.\n",
       "*   **Higher Developer Productivity:** Engineers are liberated from fire-fighting, enabling them to focus on feature development and strategic initiatives.\n",
       "*   **Enhanced Code Quality & Reduced Technical Debt:** Proactive identification and remediation of issues, coupled with Chronicler's insights into recurring patterns, prevent technical debt accumulation.\n",
       "*   **Increased System Resilience:** The system becomes inherently more capable of self-healing.\n",
       "\n",
       "---\n",
       "\n",
       "### Challenges and Mitigation Strategies:\n",
       "\n",
       "*   **Trust and Reliability:**\n",
       "    *   **Mitigation:** Human-in-the-loop for critical deployments; detailed audit trails of agent decisions; robust sandboxing for all testing; gradual increase in autonomy (e.g., start with proposing fixes, then auto-deploy for low-risk changes).\n",
       "*   **Complexity of Real-World Systems:**\n",
       "    *   **Mitigation:** Initial focus on well-architected, containerized microservices; leverage RAG with comprehensive internal documentation and architectural diagrams; continuous learning from successful resolutions.\n",
       "*   **Security:** An agent with write access to code and production is a high-value target.\n",
       "    *   **Mitigation:** Strict access controls and least privilege principles for agents; rigorous security auditing of the ABRE itself; cryptographic signing of all generated code; anomaly detection on agent behavior.\n",
       "*   **Hallucinations & Incorrect Fixes:** LLMs can generate plausible but incorrect code.\n",
       "    *   **Mitigation:** **Crucially, the Guardian Agent's robust testing and iterative feedback loop with the Craftsman Agent is the primary defense.** Multiple testing layers (unit, integration, e2e, performance, security); roll-back mechanisms.\n",
       "*   **Explainability:** Developers need to understand *why* an agent made a particular change.\n",
       "    *   **Mitigation:** ABRE generates detailed reports and comments (Pathfinder's RCA, Craftsman's justification for code, Guardian's test results) as part of the PR/deployment artifact. Chronicler's records provide a full audit trail.\n",
       "\n",
       "---\n",
       "\n",
       "The Autonomic Bug Resolution Engine (ABRE) represents a powerful shift from crisis management to intelligent, self-healing software operations, leveraging the full potential of Agentic AI to deliver unprecedented levels of reliability, efficiency, and developer satisfaction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": f\"Propose an Agentic AI solution to the {pain_point} in the {business_idea} industry.\"}]\n",
    "\n",
    "response = gemini.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "agentic_solution = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(agentic_solution))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
