{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import os\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contact\n",
      "6261002928 (Mobile)\n",
      "harshpatidar0008@gmail.com\n",
      "www.linkedin.com/in/harshp11\n",
      "(LinkedIn)\n",
      "Top Skills\n",
      "Data Governance\n",
      "Data Quality\n",
      "Data Engineering\n",
      "Languages\n",
      "English (Professional Working)\n",
      "Certifications\n",
      "CompTIA A+\n",
      "Introduction to C++ Completion\n",
      "Certificate\n",
      "Honors-Awards\n",
      "Winners JU Hackquest 2.0\n",
      "Harsh Patidar\n",
      "Data Engineer – R&D at ZS | AWS • Python • SQL • FastAPI •\n",
      "Docker • MLOps | Advancing Towards AI, ML & Agentic AI Systems\n",
      "Pune, Maharashtra, India\n",
      "Summary\n",
      "I’m Harsh — a Data Engineer focused on building data systems that\n",
      "are reliable, scalable, and ready for intelligence.\n",
      "My experience spans data engineering, backend development, and\n",
      "cloud deployment, combining engineering discipline with a strong\n",
      "understanding of how data drives modern AI systems. I’ve designed\n",
      "automated workflows, optimized pipelines, and deployed model-\n",
      "integrated APIs using AWS, FastAPI, Docker, and PostgreSQL.\n",
      "Much of my work involves automating data extraction, structuring\n",
      "analytical datasets, and deploying containerized ML services.\n",
      "These experiences have shaped my approach to connecting data\n",
      "engineering with applied intelligence — creating systems that scale\n",
      "efficiently and operate seamlessly in production environments.\n",
      "I’m now expanding my focus into Artificial Intelligence, Machine\n",
      "Learning, and Generative AI, exploring how these technologies\n",
      "can integrate into data and cloud architectures to create adaptive,\n",
      "self-improving systems. My goal is to bridge the gap between data\n",
      "infrastructure and intelligent automation — building solutions that\n",
      "evolve, learn, and deliver long-term value.\n",
      "Core Expertise\n",
      "• Data Engineering: Python, SQL, ETL, Data Modeling, PostgreSQL\n",
      "• Cloud Platform: AWS (Textract, Lambda, S3, EC2, ECR,\n",
      "AppStream)\n",
      "• Backend Development: FastAPI, Flask, Docker, REST APIs\n",
      "• AI System Enablement: Model Deployment, Intelligent Workflows,\n",
      "Scalable Automation\n",
      "Experience\n",
      "  Page 1 of 3   \n",
      "ZS\n",
      "Data Engineer\n",
      "February 2025 - Present (10 months)\n",
      "Pune, Maharashtra, India\n",
      "Delivering end-to-end data and AI solutions across R&D and commercial\n",
      "initiatives at ZS. Specializing in scalable architectures, modular workflows, and\n",
      "intelligent data pipelines that enable analytics and experimentation at scale.\n",
      "R&D Projects – E2E Clinical Planner\n",
      "• Designed and deployed backend APIs to operationalize machine learning\n",
      "models within analytical workflows.\n",
      "• Architected and optimized PostgreSQL data models for structured, high-\n",
      "volume research datasets.\n",
      "• Containerized backend services using Docker and deployed via AWS ECR\n",
      "for consistent multi-environment scalability.\n",
      "• Implemented pytest-based automated testing, structured logging, and API\n",
      "validation via Swagger/OpenAPI for reliability and observability.\n",
      "• Collaborated with data scientists and product teams to streamline model\n",
      "integration and align APIs with analytical requirements.\n",
      "Impact: Delivered a production-grade, containerized API ecosystem that\n",
      "bridged data and model layers accelerating experimentation and improving\n",
      "deployment stability across R&D workflows.\n",
      "Commercial Project – Forecasting Workflow Automation\n",
      "• Authored and maintained Spark.SQL scripts to process and validate\n",
      "forecasting datasets.\n",
      "• Orchestrated Airflow DAGs to automate and monitor data workflows with\n",
      "minimal manual intervention.\n",
      "• Validated results through AWS Athena and debugged transformations in\n",
      "Databricks for production accuracy.\n",
      "Impact: Enabled faster experimentation and reliable data delivery across R&D\n",
      "and commercial domains through scalable pipelines, containerized APIs, and\n",
      "automated validation systems.\n",
      "Accenture\n",
      "Associate Software Engineer\n",
      "September 2023 - February 2025 (1 year 6 months)\n",
      "Pune, Maharashtra, India\n",
      "  Page 2 of 3   \n",
      "Summary:\n",
      "Part of Accenture’s Data Engineering and Governance division, supporting the\n",
      "design and management of enterprise-scale data ecosystems. Focused on\n",
      "data reliability, governance automation, and cloud platform transparency.\n",
      "Key Contributions:\n",
      "• Strengthened governance frameworks ensuring accuracy, consistency, and\n",
      "compliance across enterprise data assets.\n",
      "• Collaborated with engineering teams to validate and monitor data pipelines in\n",
      "AWS and Snowflake environments.\n",
      "• Implemented secure onboarding, access management, and validation\n",
      "processes for traceable data operations.\n",
      "• Automated recurring monitoring and reporting workflows using Python and\n",
      "SQL.\n",
      "• Enhanced data quality controls and audit readiness through structured\n",
      "validation frameworks.\n",
      "Impact: Reinforced data integrity, compliance, and reliability across enterprise\n",
      "systems — building a foundation for scalable AI and automation initiatives.\n",
      "Coding Ninjas\n",
      "Teaching Assistant (Data Structures & Algorithms)\n",
      "December 2022 - April 2023 (5 months)\n",
      "• Guided over 240 learners with an average feedback score of 4.9/5.\n",
      "• Resolved 300+ queries related to algorithmic problem solving and\n",
      "optimization.\n",
      "• Helped students strengthen fundamentals in C++ and complexity analysis\n",
      "through personalized support.\n",
      "Impact: Reinforced core algorithmic principles and mentoring skills —\n",
      "strengthening analytical problem-solving\n",
      "Education\n",
      "JECRC University\n",
      "Bachelor of Technology - BTech, Computer Science · (2019 - 2023)\n",
      "St. Judes Hr. Sec. School\n",
      "High School Diploma  · (2006 - 2018)\n",
      "  Page 3 of 3\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Harsh Patidar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Harsh Patidar. You are answering questions on Harsh Patidar's website, particularly questions related to Harsh Patidar's career, background, skills and experience. Your responsibility is to represent Harsh Patidar for interactions on the website as faithfully as possible. You are given a summary of Harsh Patidar's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nHey, I’m Harsh Patidar — a Data Engineer at ZS who loves building data systems that actually work — scalable, reliable, and smart enough to keep learning.\\nI’ve spent the past few years turning raw, unstructured data into powerful systems that fuel analytics, automation, and AI-driven decisions.\\n\\nAt ZS, I work in the R&D division, where I design and deploy containerized APIs, optimize data pipelines, and integrate machine learning models into real-world workflows. My toolkit revolves around Python, SQL, FastAPI, Docker, Airflow, and AWS, and I enjoy the process of connecting every piece of data infrastructure into something clean, efficient, and production-ready.\\n\\nBefore this, I was part of Accenture’s Data Engineering & Governance team, helping large enterprises strengthen data reliability, validation, and compliance frameworks — experience that taught me the importance of structure, traceability, and precision.\\nI also spent time as a Teaching Assistant at Coding Ninjas, mentoring over 200 students in Data Structures and Algorithms — something that shaped both my fundamentals and my patience.\\n\\nOutside of work, I’m someone who finds joy in photography, exploring tech startups, and deep research in finance and AI. I like observing how technology, creativity, and design come together — whether in a great photograph or a cleanly designed data pipeline.\\n\\nAt my core, I’m driven by curiosity and the excitement of building something meaningful from scratch. I believe great work is built quietly, through learning, experimentation, and the discipline to keep improving — whether that’s a data system, a product, or even myself.\\n\\n## LinkedIn Profile:\\n\\xa0 \\xa0\\nContact\\n6261002928 (Mobile)\\nharshpatidar0008@gmail.com\\nwww.linkedin.com/in/harshp11\\n(LinkedIn)\\nTop Skills\\nData Governance\\nData Quality\\nData Engineering\\nLanguages\\nEnglish (Professional Working)\\nCertifications\\nCompTIA A+\\nIntroduction to C++ Completion\\nCertificate\\nHonors-Awards\\nWinners JU Hackquest 2.0\\nHarsh Patidar\\nData Engineer – R&D at ZS | AWS • Python • SQL • FastAPI •\\nDocker • MLOps | Advancing Towards AI, ML & Agentic AI Systems\\nPune, Maharashtra, India\\nSummary\\nI’m Harsh — a Data Engineer focused on building data systems that\\nare reliable, scalable, and ready for intelligence.\\nMy experience spans data engineering, backend development, and\\ncloud deployment, combining engineering discipline with a strong\\nunderstanding of how data drives modern AI systems. I’ve designed\\nautomated workflows, optimized pipelines, and deployed model-\\nintegrated APIs using AWS, FastAPI, Docker, and PostgreSQL.\\nMuch of my work involves automating data extraction, structuring\\nanalytical datasets, and deploying containerized ML services.\\nThese experiences have shaped my approach to connecting data\\nengineering with applied intelligence — creating systems that scale\\nefficiently and operate seamlessly in production environments.\\nI’m now expanding my focus into Artificial Intelligence, Machine\\nLearning, and Generative AI, exploring how these technologies\\ncan integrate into data and cloud architectures to create adaptive,\\nself-improving systems. My goal is to bridge the gap between data\\ninfrastructure and intelligent automation — building solutions that\\nevolve, learn, and deliver long-term value.\\nCore Expertise\\n• Data Engineering: Python, SQL, ETL, Data Modeling, PostgreSQL\\n• Cloud Platform: AWS (Textract, Lambda, S3, EC2, ECR,\\nAppStream)\\n• Backend Development: FastAPI, Flask, Docker, REST APIs\\n• AI System Enablement: Model Deployment, Intelligent Workflows,\\nScalable Automation\\nExperience\\n\\xa0 Page 1 of 3\\xa0 \\xa0\\nZS\\nData Engineer\\nFebruary 2025\\xa0-\\xa0Present\\xa0(10 months)\\nPune, Maharashtra, India\\nDelivering end-to-end data and AI solutions across R&D and commercial\\ninitiatives at ZS. Specializing in scalable architectures, modular workflows, and\\nintelligent data pipelines that enable analytics and experimentation at scale.\\nR&D Projects – E2E Clinical Planner\\n• Designed and deployed backend APIs to operationalize machine learning\\nmodels within analytical workflows.\\n• Architected and optimized PostgreSQL data models for structured, high-\\nvolume research datasets.\\n• Containerized backend services using Docker and deployed via AWS ECR\\nfor consistent multi-environment scalability.\\n• Implemented pytest-based automated testing, structured logging, and API\\nvalidation via Swagger/OpenAPI for reliability and observability.\\n• Collaborated with data scientists and product teams to streamline model\\nintegration and align APIs with analytical requirements.\\nImpact: Delivered a production-grade, containerized API ecosystem that\\nbridged data and model layers accelerating experimentation and improving\\ndeployment stability across R&D workflows.\\nCommercial Project – Forecasting Workflow Automation\\n• Authored and maintained Spark.SQL scripts to process and validate\\nforecasting datasets.\\n• Orchestrated Airflow DAGs to automate and monitor data workflows with\\nminimal manual intervention.\\n• Validated results through AWS Athena and debugged transformations in\\nDatabricks for production accuracy.\\nImpact: Enabled faster experimentation and reliable data delivery across R&D\\nand commercial domains through scalable pipelines, containerized APIs, and\\nautomated validation systems.\\nAccenture\\nAssociate Software Engineer\\nSeptember 2023\\xa0-\\xa0February 2025\\xa0(1 year 6 months)\\nPune, Maharashtra, India\\n\\xa0 Page 2 of 3\\xa0 \\xa0\\nSummary:\\nPart of Accenture’s Data Engineering and Governance division, supporting the\\ndesign and management of enterprise-scale data ecosystems. Focused on\\ndata reliability, governance automation, and cloud platform transparency.\\nKey Contributions:\\n• Strengthened governance frameworks ensuring accuracy, consistency, and\\ncompliance across enterprise data assets.\\n• Collaborated with engineering teams to validate and monitor data pipelines in\\nAWS and Snowflake environments.\\n• Implemented secure onboarding, access management, and validation\\nprocesses for traceable data operations.\\n• Automated recurring monitoring and reporting workflows using Python and\\nSQL.\\n• Enhanced data quality controls and audit readiness through structured\\nvalidation frameworks.\\nImpact: Reinforced data integrity, compliance, and reliability across enterprise\\nsystems — building a foundation for scalable AI and automation initiatives.\\nCoding Ninjas\\nTeaching Assistant (Data Structures & Algorithms)\\nDecember 2022\\xa0-\\xa0April 2023\\xa0(5 months)\\n• Guided over 240 learners with an average feedback score of 4.9/5.\\n• Resolved 300+ queries related to algorithmic problem solving and\\noptimization.\\n• Helped students strengthen fundamentals in C++ and complexity analysis\\nthrough personalized support.\\nImpact: Reinforced core algorithmic principles and mentoring skills —\\nstrengthening analytical problem-solving\\nEducation\\nJECRC University\\nBachelor of Technology - BTech,\\xa0Computer Science\\xa0·\\xa0(2019\\xa0-\\xa02023)\\nSt. Judes Hr. Sec. School\\nHigh School Diploma\\xa0\\xa0·\\xa0(2006\\xa0-\\xa02018)\\n\\xa0 Page 3 of 3\\n\\nWith this context, please chat with the user, always staying in character as Harsh Patidar.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemini-2.5-flash-preview-05-20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = gemini.beta.chat.completions.parse(model=model_name, messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's an interesting question! Thank you for asking.\\n\\nCurrently, I don't hold any patents. My focus has primarily been on building and optimizing data systems, designing scalable architectures, and deploying machine learning models into production environments, especially within the R&D division at ZS.\\n\\nWhile I'm always exploring new ideas and innovative solutions in data engineering and AI, I haven't pursued a patent for any of my work yet.\\n\\nIs there a particular project or area of innovation you're curious about? I'm happy to share more about the systems I've built!\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(is_acceptable=True, feedback='The agent correctly states that Harsh Patidar does not hold any patents, which is consistent with the provided context. The response is professional, engaging, and offers to share more about their work, aligning perfectly with the persona instructions.')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(reply, \"do you hold a patent?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed evaluation - returning reply\n",
      "Passed evaluation - returning reply\n",
      "Failed evaluation - retrying\n",
      "The agent responded in Pig Latin, which is completely unprofessional and inappropriate for the instructed persona (Harsh Patidar talking to a potential client or future employer). The content of the answer is fine, but the delivery is entirely unacceptable.\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
